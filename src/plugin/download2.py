# -*- coding: utf-8 -*-

import os
import random
import re
import sys
import json
import hashlib
import base64
import logging
from typing import Optional, Dict, List
from urllib.parse import urlparse, urljoin
from pathlib import Path

import requests
import pykeepass
from bs4 import BeautifulSoup
import urllib3
from construct import Array

# å±è”½ InsecureRequestWarning è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

HDS = [
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36  (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.105 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.88 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.62 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.97 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.5790.98 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.198 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.6045.105 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/118.0.5993.88 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/117.0.5938.62 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/116.0.5845.97 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/115.0.5790.98 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/114.0.5735.198 Safari/537.36'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.105 Safari/537.36 Edg/119.0.6045.105'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.88 Safari/537.36 Edg/118.0.5993.88'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.62 Safari/537.36 Edg/117.0.5938.62'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.97 Safari/537.36 Edg/116.0.5845.97'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.5790.98 Safari/537.36 Edg/115.0.5790.98'},
    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.198 Safari/537.36 Edg/114.0.5735.198'}
]

class FaviconDownloader:
    """Faviconä¸‹è½½å™¨ç±»"""

    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
    }

    BASE64_IMAGE_PATTERN = re.compile(r'^data:image/(png|jpeg|jpg|gif|webp|x-icon);base64')
    META_REFRESH_PATTERN = re.compile(r'URL=[\'"]?(.*?)[\'"]?', re.IGNORECASE)

    ICON_SELECTORS = [
        {'rel': 'icon'},
        {'rel': 'shortcut icon'},
        {'rel': 'apple-touch-icon'},
        {'rel': 'apple-touch-icon-precomposed'},
        {'rel': 'mask-icon'},
        {'rel': 'fluid-icon'}
    ]

    # å¸¸è§çš„faviconé»˜è®¤è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    DEFAULT_FAVICON_PATHS = [
        '/favicon',
        # assets
        '/assets/favicon',
        '/assets/img/favicon',
        '/assets/images/favicon',
        # static
        '/static/favicon',
        '/static/img/favicon',
        '/static/images/favicon',
        # image | resource
        '/img/favicon',
        '/images/favicon',
        '/public/favicon',
        '/resources/favicon',
        '/wp-content/uploads/favicon',
        '/sites/default/files/favicon',
        '/apple-touch-icon',
        '/apple-touch-icon-precomposed'
    ]

    SUPPORTED_EXTENSIONS = {'.svg', '.jpg', '.jpeg', '.png', '.webp', '.ico', '.gif'}
    MIN_FILE_SIZE = 128 * 3  # æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰

    def __init__(self, proxy: Optional[Dict] = None, apis: List[str] = None):
        self.proxy = proxy or {}
        self.apis = apis or []
        if proxy:
            for key, value in self.proxy.items():
                if "IP:PROXY" == value:
                    del self.proxy[key]
        self.has_proxy = bool(self.proxy)
        self.stats = DownloadStats()
        self.session_no_proxy = None
        self.session_with_proxy = None

        # åˆ›å»ºä¸¤ä¸ªsessionï¼šä¸€ä¸ªå¸¦ä»£ç†ï¼Œä¸€ä¸ªä¸å¸¦ä»£ç†
        logger.info("åˆ›å»ºä¸å¸¦ä»£ç†çš„ session")
        self.session_no_proxy = self._create_session()
        self.sessions = [self.session_no_proxy]
        if self.has_proxy:
            logger.info("åˆ›å»ºå¸¦ä»£ç†çš„ session")
            self.session_with_proxy = self._create_session(self.proxy)
            self.sessions.append(self.session_with_proxy)


    def _create_session(self, proxy = None) -> requests.Session:
        """åˆ›å»ºrequestsä¼šè¯"""
        session = requests.Session()
        session.headers.update(self.HEADERS)
        session.verify = False
        if proxy:
            session.proxies.update(proxy)

        return session

    def _format_url(self, url):
        """æ ¼å¼åŒ–URL"""
        if url.startswith("http://") or url.startswith("https://"):
            return url
        elif url.startswith("//"):
            return f"http:{url}"
        else:
            return f"http://{url}"

    def _make_request(self, session: requests.Session, url: str, headers=HEADERS, timeout=10, retry=2) -> Optional[requests.Response]:
        """ä½¿ç”¨æŒ‡å®šsessionå‘èµ·è¯·æ±‚"""
        try:
            logger.info(f"| å°è¯•è¯·æ±‚({3 - retry}): {url}")
            response = session.get(url, timeout=timeout, headers=headers)
            response.raise_for_status()
            logger.info(f"| è¯·æ±‚æˆåŠŸ")
            return response
        except Exception as e:
            if retry > 0:
                self._make_request(session, url, random.choice(HDS), timeout, retry - 1)
            else:
                logger.info(f"| è¯·æ±‚å¤±è´¥ {e}")
                logger.info(f"| ---------")

        return None

    def _check_connect(self, session: requests.Session, url: str, timeout=10) -> bool:
        """æ£€æŸ¥è¿æ¥æ˜¯å¦å¯ç”¨"""
        try:
            if len(session.proxies.keys()) > 0:
                logger.info(f"| å°è¯•ä»£ç†è¿æ¥: {url}")
            else:
                logger.info(f"| å°è¯•ç›´æ¥è¿æ¥: {url}")
            session.get(url, timeout=timeout)
            return True
        except:
            return False

    def _extract_redirect_url(self, html_content: str) -> Optional[str]:
        """ä»HTML metaæ ‡ç­¾ä¸­æå–é‡å®šå‘URL"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            meta_tag = soup.find('meta', attrs={'http-equiv': re.compile(r'refresh', re.I)})
            if meta_tag:
                content = meta_tag.get('content', '')
                match = self.META_REFRESH_PATTERN.search(content)
                if match:
                    return match.group(1)
        except Exception:
            logger.debug(f"| æå–é‡å®šå‘URLå¤±è´¥")
        return None

    def _find_favicon_by_page(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ä»HTMLä¸­æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„faviconå›¾ç‰‡é“¾æ¥"""
        favicon_candidates = []

        # 1. æ ‡å‡†çš„linkæ ‡ç­¾
        for selector in self.ICON_SELECTORS:
            link_tags = soup.find_all('link', attrs=selector)
            for link_tag in link_tags:
                href = link_tag.get('href')
                if href:
                    favicon_candidates.append({
                        'url': href,
                        'type': 'link_tag',
                        'rel': link_tag.get('rel', [''])[0],
                        'sizes': link_tag.get('sizes', ''),
                        'priority': 1
                    })

        # 2. metaæ ‡ç­¾ä¸­çš„å›¾ç‰‡
        meta_tags = soup.find_all('meta')
        for meta_tag in meta_tags:
            property_val = meta_tag.get('property', '').lower()
            name_val = meta_tag.get('name', '').lower()
            content = meta_tag.get('content', '')

            if property_val == 'og:image' and content:
                favicon_candidates.append({
                    'url': content,
                    'type': 'og_image',
                    'rel': 'og:image',
                    'sizes': '',
                    'priority': 3
                })
            elif name_val == 'twitter:image' and content:
                favicon_candidates.append({
                    'url': content,
                    'type': 'twitter_image',
                    'rel': 'twitter:image',
                    'sizes': '',
                    'priority': 4
                })
            elif any(keyword in name_val for keyword in ['icon', 'logo', 'image']) and content:
                favicon_candidates.append({
                    'url': content,
                    'type': 'meta_image',
                    'rel': name_val,
                    'sizes': '',
                    'priority': 5
                })

        # 3. æŸ¥æ‰¾å¯èƒ½æ˜¯logoçš„imgæ ‡ç­¾
        img_tags = soup.find_all('img')
        for img_tag in img_tags:
            src = img_tag.get('src', '')
            alt = img_tag.get('alt', '').lower()
            class_list = ' '.join(img_tag.get('class', [])).lower()
            id_attr = img_tag.get('id', '').lower()

            keywords = ['favicon', 'icon', 'logo', 'brand']
            if src and any(keyword in src.lower() for keyword in keywords):
                favicon_candidates.append({
                    'url': src,
                    'type': 'img_src',
                    'rel': 'img-src',
                    'sizes': '',
                    'priority': 6
                })
            elif any(keyword in alt for keyword in keywords):
                favicon_candidates.append({
                    'url': src,
                    'type': 'img_alt',
                    'rel': 'img-alt',
                    'sizes': '',
                    'priority': 7
                })
            elif any(keyword in class_list for keyword in keywords):
                favicon_candidates.append({
                    'url': src,
                    'type': 'img_class',
                    'rel': 'img-class',
                    'sizes': '',
                    'priority': 8
                })
            elif any(keyword in id_attr for keyword in keywords):
                favicon_candidates.append({
                    'url': src,
                    'type': 'img_id',
                    'rel': 'img-id',
                    'sizes': '',
                    'priority': 9
                })

        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶å»é‡
        seen_urls = set()
        unique_candidates = []

        for candidate in sorted(favicon_candidates, key=lambda x: x['priority']):
            url = candidate['url']
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_candidates.append(candidate)

        logger.debug(f"æ‰¾åˆ° {len(unique_candidates)} ä¸ªfaviconå€™é€‰å›¾ç‰‡")
        return unique_candidates

    def _normalize_url(self, url: str, base_url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        if self._is_base64_image(url):
            return url
        elif url.startswith('http'):
            return url
        elif url.startswith('//'):
            return f"https:{url}"
        else:
            return urljoin(base_url, url)

    def _get_file_extension(self, url: str, content_type: str = '') -> str:
        """æ ¹æ®URLå’ŒContent-Typeç¡®å®šæ–‡ä»¶æ‰©å±•å"""
        parsed = urlparse(url)
        path_ext = Path(parsed.path).suffix.lower()
        if path_ext in self.SUPPORTED_EXTENSIONS:
            return path_ext

        if content_type:
            if 'svg' in content_type:
                return '.svg'
            elif 'png' in content_type:
                return '.png'
            elif 'jpeg' in content_type or 'jpg' in content_type:
                return '.jpg'
            elif 'webp' in content_type:
                return '.webp'
            elif 'gif' in content_type:
                return '.gif'

        return '.ico'

    def _save_base64_image(self, base64_data: str, file_path: Path) -> bool:
        """ä¿å­˜base64å›¾ç‰‡"""
        try:
            _, data = base64_data.split(',', 1)
            decoded_data = base64.b64decode(data)

            if len(decoded_data) < self.MIN_FILE_SIZE:
                logger.warning(f"| Base64å›¾ç‰‡å¤ªå°ï¼Œè·³è¿‡: {len(decoded_data)} bytes")
                return False

            with open(file_path, 'wb') as f:
                f.write(decoded_data)
            logger.info(f"| æˆåŠŸä¿å­˜base64å›¾ç‰‡: {file_path}")
            return True
        except Exception:
            logger.error(f"| ä¿å­˜base64å›¾ç‰‡å¤±è´¥")
            return False

    def _is_base64_image(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºbase64å›¾ç‰‡"""
        return bool(self.BASE64_IMAGE_PATTERN.match(content))

    def _is_image_content(self, content: bytes, content_type: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºå›¾ç‰‡"""
        image_types = ['image/', 'application/octet-stream']
        if any(img_type in content_type.lower() for img_type in image_types):
            return True

        if len(content) < 4:
            return False

        magic_numbers = {
            b'\x89PNG': 'PNG',
            b'\xFF\xD8\xFF': 'JPEG',
            b'GIF8': 'GIF',
            b'RIFF': 'WEBP',
            b'\x00\x00\x01\x00': 'ICO',
            b'\x00\x00\x02\x00': 'CUR',
            b'<svg': 'SVG'
        }

        for magic, format_type in magic_numbers.items():
            if content.startswith(magic):
                logger.info(f"| æ£€æµ‹åˆ°å›¾ç‰‡æ ¼å¼: {format_type}")
                return True

        try:
            text_content = content.decode('utf-8', errors='ignore')[:1000]
            if '<svg' in text_content.lower():
                logger.info("| æ£€æµ‹åˆ°SVGæ ¼å¼")
                return True
        except:
            pass

        return False

    def _download_file(self, session: requests.Session, url: str, save_path: Path) -> bool:
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            logger.info(f"| å¼€å§‹ä¸‹è½½ favicon: {url}")

            response = self._make_request(session, url)
            if not response:
                return False

            content = response.content
            content_type = response.headers.get('Content-Type', '').lower()

            if len(content) < self.MIN_FILE_SIZE:
                logger.info(f"| æ–‡ä»¶å¤ªå°ï¼Œè·³è¿‡: {len(content)} bytes")
                return False

            if 'text/html' in content_type or not self._is_image_content(content, content_type):
                logger.info(f"| å“åº”æ˜¯HTMLé¡µé¢ï¼Œå°è¯•ä»ä¸­æå–faviconé“¾æ¥")

                try:
                    html_content = content.decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html_content, 'html.parser')
                    favicon_candidates = self._find_favicon_by_page(soup, response.url)

                    if favicon_candidates:
                        for candidate in favicon_candidates[:3]:  # åªå°è¯•å‰3ä¸ª
                            favicon_url = self._normalize_url(candidate['url'], response.url)

                            if self._is_base64_image(favicon_url):
                                return self._save_base64_image(favicon_url, save_path)

                            if favicon_url != url:
                                logger.info(f"| ä»HTMLä¸­æ‰¾åˆ°faviconé“¾æ¥: {favicon_url}")
                                return self._download_file(session, favicon_url, save_path)

                    logger.info(f"| HTMLé¡µé¢ä¸­æœªæ‰¾åˆ°faviconé“¾æ¥")
                    return False

                except Exception:
                    logger.info(f"| å¤„ç†HTMLå†…å®¹å¤±è´¥")
                    return False

            logger.info(f"| æ£€æµ‹åˆ°å›¾ç‰‡å†…å®¹: {content_type}")

            ext = self._get_file_extension(url, content_type)
            final_path = save_path.with_suffix(ext)

            with open(final_path, 'wb') as f:
                f.write(content)

            logger.info(f"| æˆåŠŸä¸‹è½½favicon: {final_path}")
            return True

        except Exception:
            logger.info(f"| ä¿å­˜æ–‡ä»¶å¤±è´¥ {url}")
            return False

    def download_favicon(self, url: str, save_path: Path) -> bool:
        """ä¸‹è½½faviconçš„ä¸»å…¥å£"""
        logger.info(f"â”Œ{"â”€" * 60}")
        logger.info(f"| å¼€å§‹æŠ“å–ç«™ç‚¹ favicon : {url}")
        logger.info(f"â”œ{"â”€" * 60}")
        download_result = self._download_favicon_inner(url, save_path)
        if download_result:
            logger.info(f"| ä¸‹è½½æˆåŠŸ: {url}")
        else:
            self.stats.add_failure(url, "æ‰€æœ‰æ–¹æ³•å¤±è´¥")
            logger.info(f"| ä¸‹è½½å¤±è´¥: {url}")
        logger.info(f"â””{"â”€" * 60}\n")
        return download_result

    def _download_favicon_inner(self, url: str, save_path: Path) -> bool:
        """ä¸‹è½½faviconçš„å†…éƒ¨å®ç°"""
        if not url:
            return False

        save_path.parent.mkdir(parents=True, exist_ok=True)

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½•æ‰©å±•åçš„æ–‡ä»¶
        existing_files = list(save_path.parent.glob(f"{save_path.stem}.*"))
        if existing_files:
            logger.info(f"| æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {existing_files[0]}")
            self.stats.add_skip()
            return True

        # è§£æURL
        url = self._format_url(url)
        parsed_url = urlparse(url)
        domain = parsed_url.netloc or parsed_url.path
        if not domain:
            logger.info(f"| æ— æ•ˆURL: {url}")
            self.stats.add_failure(url, "æ— æ•ˆURL")
            return False

        # æ£€æµ‹å¯ç”¨çš„ session å’Œåè®®
        session = None
        final_url = None
        logger.info(f"| å°è¯•è¿æ¥ç«™ç‚¹")

        for scheme in ["http", "https"]:
            for test_session in self.sessions:
                if test_session is None:
                    continue
                new_url = f"{scheme}://{domain}{parsed_url.path}"
                if self._check_connect(test_session, new_url):
                    session = test_session
                    final_url = new_url
                    break
            if session:
                break

        if not session or not final_url:
            logger.info(f"| è¿æ¥ç«™ç‚¹å¤±è´¥")
            self.stats.add_failure(url, "è¿æ¥å¤±è´¥")
            return False

        # æ–¹æ³•1: å°è¯•é»˜è®¤è·¯å¾„
        logger.info(f"â”œâ”€â”€â”€â”€â”€â”€â”€ æ–¹æ³•1 : å°è¯•é»˜è®¤faviconè·¯å¾„ â”€â”€â”€â”€â”€â”€â”€")
        if self._try_favicon_from_paths(session, final_url, save_path):
            self.stats.add_success('direct_favicon')
            return True

        # æ–¹æ³•2: ä»ç½‘ç«™é¡µé¢è·å–
        logger.info(f"â”œâ”€â”€â”€â”€â”€â”€â”€ æ–¹æ³•2 : ä»ç½‘ç«™é¡µé¢è§£æ â”€â”€â”€â”€â”€â”€â”€")
        if self._try_favicon_from_page_content(session, final_url, save_path):
            return True

        # æ–¹æ³•3: ä½¿ç”¨ç¬¬ä¸‰æ–¹API
        logger.info(f"â”œâ”€â”€â”€â”€â”€â”€â”€ æ–¹æ³•3 : ä½¿ç”¨ç¬¬ä¸‰æ–¹API ({len(self.apis)}) â”€â”€â”€â”€â”€â”€â”€")
        if len(self.apis) > 0:
            if self.session_with_proxy:
                session = self.session_with_proxy
            if self._try_favicon_from_apis(session, domain, save_path):
                return True

        return False

    def _try_favicon_from_paths(self, session: requests.Session, base_url: str, save_path: Path) -> bool:
        """å°è¯•å¸¸è§çš„é»˜è®¤faviconè·¯å¾„"""
        logger.info(f"| å°è¯•é»˜è®¤faviconè·¯å¾„: {base_url}")
        for path in self.DEFAULT_FAVICON_PATHS:
            for ext in self.SUPPORTED_EXTENSIONS:
                favicon_url = urljoin(base_url, f"{path}{ext}")
                logger.info(f"| å°è¯•é»˜è®¤è·¯å¾„: {favicon_url}")

                if self._download_file(session, favicon_url, save_path):
                    logger.info(f"| é»˜è®¤è·¯å¾„æˆåŠŸ: {favicon_url}")
                    return True

        logger.info("| æ‰€æœ‰é»˜è®¤è·¯å¾„éƒ½å¤±è´¥äº†")
        return False

    def _try_favicon_from_page_content(self, session: requests.Session, url: str, save_path: Path) -> bool:
        """ä»é¡µé¢å†…å®¹è§£æå¹¶ä¸‹è½½favicon"""
        try:
            logger.info(f"| ä»é¡µé¢è·å–favicon: {url}")

            response = self._make_request(session, url)
            if not response:
                self.stats.add_failure(url, "é¡µé¢è¯·æ±‚å¤±è´¥")
                return False

            # å¤„ç†é‡å®šå‘
            final_url = response.url
            redirect_url = self._extract_redirect_url(response.text)
            if redirect_url:
                final_url = urljoin(final_url, redirect_url)
                logger.info(f"| å‘ç°é‡å®šå‘: {final_url}")
                response = self._make_request(session, final_url)
                if not response:
                    self.stats.add_failure(url, "é‡å®šå‘é¡µé¢è¯·æ±‚å¤±è´¥")
                    return False

            # è§£æHTMLæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„faviconå›¾ç‰‡
            soup = BeautifulSoup(response.text, 'html.parser')
            favicon_candidates = self._find_favicon_by_page(soup, final_url)

            if favicon_candidates:
                for candidate in favicon_candidates:
                    favicon_url = candidate['url']
                    candidate_type = candidate['type']

                    logger.debug(f"| å°è¯•å€™é€‰å›¾ç‰‡ [{candidate_type}]: {favicon_url}")

                    if self._is_base64_image(favicon_url):
                        success = self._save_base64_image(favicon_url, save_path)
                        if success:
                            self.stats.add_success('base64_image')
                            return True
                        continue

                    normalized_url = self._normalize_url(favicon_url, final_url)
                    success = self._download_file(session, normalized_url, save_path)
                    if success:
                        self.stats.add_success('from_html_page')
                        logger.info(f"| æˆåŠŸä¸‹è½½ [{candidate_type}]: {normalized_url}")
                        return True
                    else:
                        logger.info(f"| å€™é€‰å›¾ç‰‡ä¸‹è½½å¤±è´¥ [{candidate_type}]: {normalized_url}")

            logger.info("| æ‰€æœ‰å€™é€‰å›¾ç‰‡éƒ½å¤±è´¥ï¼Œå°è¯•é»˜è®¤faviconè·¯å¾„")
            success = self._try_favicon_from_paths(session, final_url, save_path)
            if success:
                self.stats.add_success('default_paths')
                return True

            self.stats.add_failure(url, "é¡µé¢è§£æå’Œé»˜è®¤è·¯å¾„éƒ½å¤±è´¥")
            return False

        except Exception as e:
            logger.error(f"ä»é¡µé¢è·å–faviconå¤±è´¥ {url}: {e}")
            self.stats.add_failure(url, f"é¡µé¢è§£æå¼‚å¸¸: {str(e)}")
            return False

    def _try_favicon_from_apis(self, session: requests.Session, domain: str, save_path: Path) -> bool:
        """ä½¿ç”¨ç¬¬ä¸‰æ–¹APIä¸‹è½½favicon"""
        for api in self.apis:
            try:
                api_url = api.replace('{domain}', domain)
                logger.info(f"| å°è¯•API: {api_url}")

                success = self._download_file(session, api_url, save_path)
                if success:
                    self.stats.add_success('from_api')
                    return True
            except Exception:
                logger.info(f"| APIä¸‹è½½å¤±è´¥ {api_url}")
                continue

        self.stats.add_failure(f"domain:{domain}", "æ‰€æœ‰APIæ–¹æ³•å¤±è´¥")
        return False


class DownloadStats:
    """ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯ç±»"""

    def __init__(self):
        self.total_count = 0
        self.success_count = 0
        self.skip_count = 0
        self.failed_urls = []
        self.failed_reasons = {}

        self.success_methods = {
            'direct_favicon': 0,
            'from_html_page': 0,
            'default_paths': 0,
            'from_api': 0,
            'base64_image': 0
        }

    def add_success(self, method: str = 'unknown'):
        """è®°å½•æˆåŠŸ"""
        self.success_count += 1
        if method in self.success_methods:
            self.success_methods[method] += 1

    def add_skip(self):
        """è®°å½•è·³è¿‡"""
        self.skip_count += 1

    def add_failure(self, url: str, reason: str):
        """è®°å½•å¤±è´¥"""
        self.failed_urls.append(url)
        if reason in self.failed_reasons:
            self.failed_reasons[reason] += 1
        else:
            self.failed_reasons[reason] = 1

    def add_total(self):
        """å¢åŠ æ€»æ•°"""
        self.total_count += 1

    @property
    def failure_count(self) -> int:
        """å¤±è´¥æ•°é‡"""
        return len(self.failed_urls)

    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        attempted = self.total_count - self.skip_count
        if attempted == 0:
            return 0.0
        return (self.success_count / attempted) * 100

    @property
    def overall_completion_rate(self) -> float:
        """æ€»å®Œæˆç‡"""
        if self.total_count == 0:
            return 0.0
        return ((self.success_count + self.skip_count) / self.total_count) * 100

    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        summary = []
        summary.append("=" * 60)
        summary.append("ğŸ“Š ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š")
        summary.append("=" * 60)
        summary.append(f"ğŸ“ˆ æ€»å¤„ç†æ•°é‡: {self.total_count}")
        summary.append(f"âœ… æˆåŠŸä¸‹è½½: {self.success_count}")
        summary.append(f"â­ï¸ è·³è¿‡(å·²å­˜åœ¨): {self.skip_count}")
        summary.append(f"âŒ ä¸‹è½½å¤±è´¥: {self.failure_count}")
        summary.append(f"ğŸ“Š æˆåŠŸç‡: {self.success_rate:.1f}% (ä¸å«è·³è¿‡)")
        summary.append(f"ğŸ“Š æ€»å®Œæˆç‡: {self.overall_completion_rate:.1f}% (å«è·³è¿‡)")

        if self.success_count > 0:
            summary.append("\nğŸ¯ æˆåŠŸæ–¹å¼åˆ†å¸ƒ:")
            for method, count in self.success_methods.items():
                if count > 0:
                    percentage = (count / self.success_count) * 100
                    method_name = {
                        'direct_favicon': 'ç›´æ¥/favicon.ico',
                        'from_html_page': 'HTMLé¡µé¢è§£æ',
                        'default_paths': 'é»˜è®¤è·¯å¾„å°è¯•',
                        'from_api': 'ç¬¬ä¸‰æ–¹API',
                        'base64_image': 'Base64å›¾ç‰‡'
                    }.get(method, method)
                    summary.append(f"  â€¢ {method_name}: {count} ({percentage:.1f}%)")

        if self.failed_reasons:
            summary.append("\nğŸ’¥ å¤±è´¥åŸå› åˆ†å¸ƒ:")
            for reason, count in sorted(self.failed_reasons.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / self.failure_count) * 100
                summary.append(f"  â€¢ {reason}: {count} ({percentage:.1f}%)")

        if self.failed_urls:
            summary.append(f"\nâŒ å¤±è´¥çš„URLåˆ—è¡¨ (å‰{min(10, len(self.failed_urls))}ä¸ª):")
            for i, url in enumerate(self.failed_urls[:10], 1):
                summary.append(f"  {i:2d}. {url}")
            if len(self.failed_urls) > 10:
                summary.append(f"     ... è¿˜æœ‰ {len(self.failed_urls) - 10} ä¸ªå¤±è´¥çš„URL")

        summary.append("=" * 60)
        return "\n".join(summary)

    def save_failed_urls(self, file_path: Path):
        """ä¿å­˜å¤±è´¥çš„URLåˆ°æ–‡ä»¶"""
        if not self.failed_urls:
            return

        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("# ä¸‹è½½å¤±è´¥çš„URLåˆ—è¡¨\n")
                f.write(f"# æ€»å¤±è´¥æ•°é‡: {len(self.failed_urls)}\n\n")
                for url in self.failed_urls:
                    f.write(f"{url}\n")

            logger.info(f"å¤±è´¥URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥URLåˆ—è¡¨æ—¶å‡ºé”™: {e}")


class KeePassProcessor:
    """KeePassæ•°æ®åº“å¤„ç†å™¨"""

    def __init__(self, downloader: FaviconDownloader):
        self.downloader = downloader

    def process_database(self, db_config: Dict, save_dir: Path) -> int:
        """å¤„ç†å•ä¸ªæ•°æ®åº“"""
        db_path = db_config['path']
        password = db_config['password']

        if not os.path.exists(db_path):
            logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            return 0

        success_count = 0

        try:
            with pykeepass.PyKeePass(db_path, password=password) as kdb:
                logger.info(f"å¤„ç†æ•°æ®åº“: {db_path}\n")
                for entry in kdb.entries:
                    url = entry.url
                    if not url:
                        continue

                    self.downloader.stats.add_total()

                    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + '.ico'
                    save_path = save_dir / filename

                    if self.downloader.download_favicon(url, save_path):
                        success_count += 1

        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®åº“å¤±è´¥ {db_path}: {e}")

        return success_count

    def process_all_databases(self, config: Dict, save_dir: Path) -> int:
        """å¤„ç†æ‰€æœ‰æ•°æ®åº“"""
        total_success = 0
        for db_config in config.get('databases', []):
            success_count = self.process_database(db_config, save_dir)
            total_success += success_count
            logger.info(f"æ•°æ®åº“å¤„ç†å®Œæˆï¼ŒæˆåŠŸä¸‹è½½: {success_count} ä¸ªå›¾æ ‡")

        return total_success


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python script.py <é…ç½®æ–‡ä»¶> <ä¿å­˜ç›®å½•>")
        sys.exit(1)

    config_file = sys.argv[1]
    save_dir = Path(sys.argv[2])
    logger.info("å¼€å§‹è¯»å–é…ç½®")

    if not os.path.exists(config_file):
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        sys.exit(1)

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"config : {config}")
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)

    # åˆ›å»ºä¸‹è½½å™¨å’Œå¤„ç†å™¨
    proxy = config.get('proxy', {})
    logger.info(f"proxy : {proxy}")
    apis = config.get('favicon_apis', {})
    logger.info(f"apis : {apis}")
    downloader = FaviconDownloader(proxy=proxy, apis=apis)
    processor = KeePassProcessor(downloader)

    logger.info("å¼€å§‹å¤„ç†...")
    total_success = processor.process_all_databases(config, save_dir)

    print(downloader.stats.get_summary())

    failed_urls_file = save_dir / 'failed_urls.txt'
    downloader.stats.save_failed_urls(failed_urls_file)

    logger.info(f"å¤„ç†å®Œæˆï¼æ€»å…±æˆåŠŸä¸‹è½½: {total_success} ä¸ªå›¾æ ‡")


if __name__ == "__main__":
    main()